{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# それぞれのスレのレスを取得\n",
    "スレタイにキーワード該当がある場合はThreadsに全レスを記録  \n",
    "レスにキーワード該当がある場合はResponsesにそのレスを記録（上記の場合も含む）  \n",
    "途中で中断しても、全実行で続きからスクレイピングすることができる  \n",
    "リセットしたい場合はProgresses/Threadsにあるファイルを消去←このファイルが消去されている場合はスクレイピング結果がリセットされて最初からやりなおされるため注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ管理のID\n",
    "ID = \"mimikkyu\"\n",
    "\n",
    "#リンク保存するファイル\n",
    "links_file = \"Links/%s.txt\"%ID\n",
    "\n",
    "#スレ保存するファイル\n",
    "threads_file = \"Threads/%s.txt\"%ID\n",
    "\n",
    "#レス保存するファイル\n",
    "responses_file = \"Responses/%s.txt\"%ID\n",
    "\n",
    "#スレタイを保存するファイル\n",
    "titles_file = \"Titles/%s.txt\"%ID\n",
    "\n",
    "#進捗ファイル\n",
    "progress_file = \"Progresses/Threads/%s.txt\"%ID\n",
    "\n",
    "#キーワードファイル\n",
    "keywords_file = \"Keywords/%s.txt\"%ID\n",
    "\n",
    "#無条件で保存するか（母集団取得のため）\n",
    "GET_POPULATION = True\n",
    "population_file = \"Responses/%s-population.txt\"%(ID)\n",
    "\n",
    "#並列数\n",
    "WORKERS_N = 30\n",
    "\n",
    "#スクレイピングするファイル数（全スレ抽出する場合はNoneに）\n",
    "WANTED_THREADS = 50000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 進捗を確認し、スクレイピングする順番を決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#対象リストを取得\n",
    "with open(links_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    links = f.read().split()\n",
    "\n",
    "#進捗を確認\n",
    "processed = []\n",
    "try:\n",
    "    with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        processed = list(map(int, f.read().split()))\n",
    "except:\n",
    "    #進捗ファイルがない（初回）\n",
    "    with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        processed = []\n",
    "\n",
    "#順番を決定\n",
    "order = []\n",
    "for cnt in range(len(links)):\n",
    "    if cnt not in processed:\n",
    "        #未スクレイピング\n",
    "        order.append(cnt)\n",
    "\n",
    "#シャッフル\n",
    "random.shuffle(order)\n",
    "\n",
    "len(processed), len(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ランダム抽出\n",
    "if WANTED_THREADS != None:\n",
    "    order = random.sample(order, min(WANTED_THREADS, len(order)))\n",
    "\n",
    "len(order)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スレ・レス・スレタイファイルを用意\n",
    "  \n",
    "スレ・レスの形式は  \n",
    "Y/M/D|H/M/S <\\t> リンクインデックス <\\t> レス番号 <\\t> レス内容  \n",
    "  \n",
    "スレタイの形式は  \n",
    "リンクインデックス <\\t> スレタイ\n",
    "　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(processed) == 0:\n",
    "    with open(threads_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"date\\tlink_index\\tnumber\\tcontent\\n\")\n",
    "\n",
    "    with open(responses_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"date\\tlink_index\\tnumber\\tcontent\\n\")\n",
    "\n",
    "    with open(population_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"date\\tlink_index\\tnumber\\tcontent\\n\")\n",
    "    \n",
    "    with open(titles_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"link_index\\ttitle\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## キーワードを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ミミッキュ']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = []\n",
    "with open(keywords_file) as f:\n",
    "    keywords = f.read().split()\n",
    "\n",
    "keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "from time import sleep\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleniumブラウザを用意する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = []\n",
    "\n",
    "class Driver:\n",
    "    def __init__(self, option):\n",
    "        self.driver = webdriver.Chrome(options=option)\n",
    "        self.used = False\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "for _ in range(WORKERS_N):\n",
    "    drivers.append(Driver(chrome_options))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定のページからスクレイピング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安価先の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '5', '76'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \" >>1 >>5 >>76 キキョウシティのBGMすこ\"\n",
    "\n",
    "def get_ankers(text):\n",
    "    result = re.findall(r\"\\>\\>\\d+\", text, re.S)\n",
    "    \n",
    "    numbers = []\n",
    "    for anker in result:\n",
    "        numbers.append(str(anker[2:]))\n",
    "\n",
    "    return set(numbers)\n",
    "\n",
    "get_ankers(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 投稿内容の処理 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(content):\n",
    "    result = content.replace(\"\\t\", \"<\\\\t>\").replace(\"\\n\", \"<\\\\br>\")\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各URLに対する処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textにキーワードがあったら\n",
    "def has_keyword(text):\n",
    "    for keyword in keywords:\n",
    "        if keyword in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#返り値：メタ情報、コンテント\n",
    "def get_from_current(soup):\n",
    "    thread_div = soup.find(\"div\", class_=\"thread\")\n",
    "    posts_div = thread_div.find_all(\"div\", class_=\"post\")\n",
    "\n",
    "    metas = []\n",
    "    contents = []\n",
    "\n",
    "    for post_div in posts_div:\n",
    "        #メタ情報\n",
    "        number = post_div.find(\"span\", class_=\"number\").text\n",
    "        \n",
    "        date = post_div.find(\"span\", class_=\"date\").text\n",
    "        date = get_time(date)\n",
    "        \n",
    "        metas.append((number, date))\n",
    "\n",
    "        #内容\n",
    "        content = post_div.find(\"div\", class_=\"message\").text\n",
    "        contents.append(content)\n",
    "\n",
    "    return (metas, contents)\n",
    "\n",
    "def get_from_past(soup):\n",
    "    #投稿内容\n",
    "    dd = soup.find_all(\"dd\")\n",
    "    dd[0].find(\"div\").decompose()\n",
    "\n",
    "    #メタ情報\n",
    "    dt = soup.find_all(\"dt\")\n",
    "\n",
    "    metas = []\n",
    "    contents = []\n",
    "    for cnt in range(len(dd)):\n",
    "        #メタ情報\n",
    "        meta = str(dt[cnt])\n",
    "        #投稿番号\n",
    "        result = re.match(r'<dt>\\d+', meta, re.S)\n",
    "        if result != None:\n",
    "            number = result.group(0).split(\"<dt>\")[1]\n",
    "        else:\n",
    "            number = None\n",
    "\n",
    "        #投稿時間\n",
    "        date = get_time(meta)\n",
    "        \n",
    "        metas.append((number, date))\n",
    "\n",
    "        #投稿内容\n",
    "        content = dd[cnt].text      \n",
    "        contents.append(content)\n",
    "\n",
    "    return metas, contents  \n",
    "\n",
    "def get_time(string):\n",
    "    try:\n",
    "        #日付\n",
    "        result = re.search(r\"\\d{4}\\/\\d{2}\\/\\d{2}\", string, re.S)\n",
    "        if result == None:\n",
    "            result = re.search(r\"\\d{2}\\/\\d{2}\\/\\d{2}\", string, re.S)\n",
    "            date_string = \"20\"+result.group(0)\n",
    "        else:\n",
    "            date_string = result.group(0)\n",
    "\n",
    "        #時間\n",
    "        result = re.search(r\"\\d{2}\\:\\d{2}\", string, re.S)\n",
    "\n",
    "        time_string = result.group(0)\n",
    "\n",
    "        date = datetime.datetime.strptime(date_string+\"|\"+time_string, \"%Y/%m/%d|%H:%M\")\n",
    "  \n",
    "        return date\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル記入重複防止\n",
    "write_lock = Lock()\n",
    "\n",
    "#レス、スレファイルへの記入\n",
    "def write_response(f, time, thread_index, response_index, content):\n",
    "    if time != None:\n",
    "        _time = time.strftime(\"%Y/%m/%d|%H/%M/%S\")\n",
    "    else:\n",
    "        _time = None\n",
    "    f.write(str(_time)+\"\\t\"\n",
    "            +str(thread_index)+\"\\t\"\n",
    "            +str(response_index)+\"\\t\"\n",
    "            +content+\"\\n\")\n",
    "\n",
    "def write_titles(f, link_index, title):\n",
    "    f.write(str(link_index)+\"\\t\"\n",
    "            +title+\"\\n\")\n",
    "\n",
    "#使われていないブラウザを返す\n",
    "def GetUnusedDriver():\n",
    "    global drivers\n",
    "    \n",
    "    for driver in drivers:\n",
    "        if driver.used == False:\n",
    "            return driver\n",
    "    \n",
    "    print(\"ALL USED\")\n",
    "\n",
    "def scrape(link, link_index):\n",
    "    global write_lock\n",
    "    #ブラウザを使い始める\n",
    "    driver = GetUnusedDriver()\n",
    "    if driver != None:\n",
    "        driver.used = True\n",
    "    else:\n",
    "        #ブラウザがなかった\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        #アクセス\n",
    "        while True:\n",
    "            driver.driver.get(link)\n",
    "\n",
    "            sleep(0.5)\n",
    "\n",
    "            html = driver.driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            #人大杉対策\n",
    "            title = soup.find(\"title\").text\n",
    "            \n",
    "            if (\"error\" in title):\n",
    "                #時間をおいてやり直し\n",
    "                sleep(1)\n",
    "            else:\n",
    "                #アクセス完了\n",
    "                break\n",
    "\n",
    "        #現在型か過去型か判別\n",
    "        if soup.find_all(\"meta\")[1].get(\"property\") == \"og:title\":\n",
    "            current = False\n",
    "        else:\n",
    "            current = True\n",
    "\n",
    "        #タイトルにキーワードがある場合はThreadsに全レスを記入\n",
    "        threads = has_keyword(title)\n",
    "        \n",
    "        #スクレイピング\n",
    "        if current:\n",
    "            metas, contents = get_from_current(soup)\n",
    "        else:\n",
    "            metas, contents = get_from_past(soup)\n",
    "\n",
    "        #同時書き込み防止\n",
    "        with write_lock:\n",
    "            #処理済みを記入\n",
    "            with open(progress_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(str(link_index) + \"\\n\")\n",
    "\n",
    "            #タイトル記入\n",
    "            with open(titles_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                write_titles(f, link_index, title)\n",
    "\n",
    "            #スレ・レス記入\n",
    "            n = len(metas)\n",
    "            found_keyword = []\n",
    "            \n",
    "            with open(threads_file, \"a\", encoding=\"utf-8\") as threads_f:\n",
    "                with open(responses_file, \"a\", encoding=\"utf-8\") as responses_f:\n",
    "                    with open(population_file, \"a\", encoding=\"utf-8\") as population_f:\n",
    "                        for cnt in range(n):\n",
    "                            content = process_content(contents[cnt])\n",
    "                            number, time = metas[cnt]\n",
    "\n",
    "                            #母集団記録モード\n",
    "                            if GET_POPULATION:\n",
    "                                write_response(population_f, time, link_index, number, content)\n",
    "\n",
    "                            #スレへの記入\n",
    "                            if threads:\n",
    "                                write_response(threads_f, time, link_index, number, content)\n",
    "\n",
    "                            #レスへの記入\n",
    "                            #レスにキーワードがあったら or 安価先に言及済みがあったら\n",
    "                            if has_keyword(content) | (len(get_ankers(content) & set(found_keyword)) > 0):\n",
    "                                found_keyword.append(number) \n",
    "                                write_response(responses_f, time, link_index, number, content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(link)\n",
    "        print(str(e))\n",
    "\n",
    "    #使い終わった\n",
    "    driver.used = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 並列スクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS_N) as executor:\n",
    "    futures = [executor.submit(scrape, links[index], index) for index in order]\n",
    "\n",
    "    #完了まで待つ\n",
    "    _ = concurrent.futures.as_completed(fs = futures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af19612f8f38f6812d87a5ef9419a460d9700d7e1cf8ff071cf581e2ee4d6233"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
