{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5chから母集団となるスレのURLを収集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#スレのリンク進捗\n",
    "lists_file = \"Progresses/Population/lists.txt\"\n",
    "\n",
    "#スレのリンク保存するファイル\n",
    "links_file = \"Links/population.txt\"\n",
    "\n",
    "#過去ログ一覧\n",
    "kakolog_link = \"https://medaka.5ch.net/kakolog_servers.html\"\n",
    "\n",
    "#並列数\n",
    "WORKERS_N = 30\n",
    "\n",
    "#取得したいリスト数\n",
    "wanted_lists = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 過去ログ一覧からターゲットのリンクをランダム選出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "from time import sleep\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ブラウザを用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = []\n",
    "\n",
    "class Driver:\n",
    "    def __init__(self, option):\n",
    "        self.driver = webdriver.Chrome(options=option)\n",
    "        self.used = False\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "for _ in range(WORKERS_N):\n",
    "    drivers.append(Driver(chrome_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ひとまず一つだけ使用\n",
    "driver = drivers[0].driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#すでにリストURLを取得していたら、リストURL収集をスキップ\n",
    "try:\n",
    "    #すでに収集した\n",
    "    with open(lists_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        list_urls = f.read().split()\n",
    "except:\n",
    "    #してなかった\n",
    "    list_urls = []\n",
    "\n",
    "if len(list_urls) > 0:\n",
    "    print(list_urls[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 過去ログからサーバーリンク一覧を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_links = []\n",
    "\n",
    "driver.get(kakolog_link)\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "tds = soup.find_all(\"td\")\n",
    "for td in tds:\n",
    "    link = td.find(\"a\").get(\"href\")\n",
    "    server_links.append(link)\n",
    "\n",
    "server_links[:5], len(server_links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サーバーページから板を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_links = []\n",
    "\n",
    "if list_urls == []:\n",
    "    for server_link in server_links:\n",
    "        driver.get(server_link)\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        boards = soup.find_all(\"p\", class_=\"board_even\")\n",
    "        boards.extend(soup.find_all(\"p\", class_ = \"board_odd\"))\n",
    "\n",
    "        for board in boards:\n",
    "            link_head = board.find(\"a\").get(\"href\")\n",
    "            link_base = \"/\".join(server_link.split(\"/\")[:3])\n",
    "            board_links.append(link_base+link_head)\n",
    "\n",
    "board_links[:5], len(board_links)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 板ページからリストURLを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使われていないブラウザを返す\n",
    "def GetUnusedDriver():\n",
    "    global drivers\n",
    "    \n",
    "    for driver in drivers:\n",
    "        if driver.used == False:\n",
    "            return driver\n",
    "    \n",
    "    print(\"ALL USED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_lock = Lock()\n",
    "\n",
    "def scrape(board_link):\n",
    "    global write_lock\n",
    "\n",
    "    driver = GetUnusedDriver()\n",
    "    driver.used = True\n",
    "\n",
    "    list_urls = []\n",
    "\n",
    "    try:\n",
    "        #自分自身\n",
    "        list_urls.append(board_link)\n",
    "\n",
    "        driver.driver.get(board_link)\n",
    "\n",
    "        html = driver.driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        #左のメニューより取得\n",
    "        menu_div = soup.find(\"div\", class_ = \"menu\")\n",
    "\n",
    "        for p in menu_div.find_all(\"p\", class_=\"menu_link\"):\n",
    "            a = p.find(\"a\")\n",
    "            #「このサーバー」ではない\n",
    "            if a != None: \n",
    "                if a.get(\"href\")[0] == \".\":\n",
    "                    #Other Listである\n",
    "                    link = board_link + a.get(\"href\")[2:]\n",
    "                    list_urls.append(link)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        #記録\n",
    "        with write_lock:\n",
    "            with open(lists_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                for list_url in list_urls:\n",
    "                    f.write(list_url + \"\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(board_link)\n",
    "        print(str(e))\n",
    "    \n",
    "    driver.used = False\n",
    "\n",
    "    return list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(board_links)\n",
    "\n",
    "if list_urls == []:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS_N) as executor:\n",
    "        futures = [executor.submit(scrape, link) for link in board_links]\n",
    "\n",
    "        #完了まで待つ\n",
    "        for future in concurrent.futures.as_completed(fs = futures):\n",
    "            list_urls.extend(future.result())\n",
    "\n",
    "list_urls[:9], len(list_urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各リストからスレURLを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_lock = Lock()\n",
    "\n",
    "def scrape(list_url):\n",
    "    global write_lock\n",
    "\n",
    "    driver = GetUnusedDriver()\n",
    "    driver.used = True\n",
    "\n",
    "    output = []\n",
    "\n",
    "    try:\n",
    "        driver.driver.get(list_url)\n",
    "\n",
    "        html = driver.driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        for p in soup.find_all(\"p\", class_ = \"main_odd\"):\n",
    "            href = p.find(\"a\").get(\"href\")\n",
    "\n",
    "            #URLを組み立てる\n",
    "            link = \"https://\" + list_url.split(\"/\")[2] +href\n",
    "\n",
    "            output.append(link) \n",
    "\n",
    "        for p in soup.find_all(\"p\", class_ = \"main_even\"):\n",
    "            href = p.find(\"a\").get(\"href\")\n",
    "\n",
    "            #URLを組み立てる\n",
    "            link = \"https://\" + list_url.split(\"/\")[2] +href\n",
    "\n",
    "            output.append(link) \n",
    "\n",
    "        with write_lock:\n",
    "            with open(links_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                for thread_link in output:\n",
    "                    f.write(thread_link + \"\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(list_url)\n",
    "        print(str(e))\n",
    "    \n",
    "    driver.used = False\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#リストをランダムに抽出\n",
    "target_lists = random.sample(list_urls, wanted_lists)\n",
    "\n",
    "thread_links = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS_N) as executor:\n",
    "    futures = [executor.submit(scrape, url) for url in list_urls]\n",
    "\n",
    "    #完了まで待つ\n",
    "    for future in concurrent.futures.as_completed(fs = futures):\n",
    "        thread_links.extend(future.result())\n",
    "\n",
    "thread_links[:5], len(thread_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af19612f8f38f6812d87a5ef9419a460d9700d7e1cf8ff071cf581e2ee4d6233"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
